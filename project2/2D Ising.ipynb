{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add project 1 to sys path \n",
    "import sys\n",
    "sys.path.append('../project1/')\n",
    "import tools as proj1_tools\n",
    "import project2_tools as proj2_tools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 40\n",
    "temps = proj2_tools.get_available_t()\n",
    "states = proj2_tools.read_t(temps[0])\n",
    "#states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two dimensional Ising Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(n, L,t_crit):\n",
    "    \"\"\"Pick some random training data from whole data set\"\"\"\n",
    "    Nt = temps.size\n",
    "    Nl = L * L\n",
    "    data = np.zeros((Nt, n, Nl),dtype=np.int8)\n",
    "    for i,t in enumerate(temps):\n",
    "        d = proj2_tools.read_t(t)\n",
    "        d[np.where(d == 0)] = -1\n",
    "        data[i] = d\n",
    "        \n",
    "    n_ordered = np.sum(temps > t_crit)\n",
    "    classes = np.zeros((Nt,n),dtype = np.int8)\n",
    "    classes[:-n_ordered] = 1\n",
    "    \n",
    "    data = data.reshape(-1, Nl)\n",
    "    \n",
    "    classes = classes.ravel()\n",
    "    return data, classes\n",
    "\n",
    "    \n",
    "n = 10000\n",
    "L = 40\n",
    "data,classes = get_train_data(n, L, 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 1000\n",
    "\n",
    "np.random.seed(0)\n",
    "indx = np.arange(data.shape[0])\n",
    "np.random.shuffle(indx)\n",
    "\n",
    "X_train = data[indx][:N_train]\n",
    "X_train = np.c_[np.ones_like(X_train[:,0]), X_train]\n",
    "y_train = classes[indx][:N_train]\n",
    "\n",
    "X_test = data[indx][N_train:2*N_train]\n",
    "X_test = np.c_[np.ones_like(X_test[:,0]), X_test]\n",
    "y_test = classes[indx][N_train:2*N_train]\n",
    "\n",
    "X_test2 = data[indx][2*N_train:3*N_train]\n",
    "X_test2 = np.c_[np.ones_like(X_test2[:,0]), X_test2]\n",
    "y_test2 = classes[indx][2*N_train:3*N_train]\n",
    "\n",
    "X_test3 = data[indx][3*N_train:4*N_train]\n",
    "X_test3 = np.c_[np.ones_like(X_test3[:,0]), X_test3]\n",
    "y_test3 = classes[indx][3*N_train:4*N_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "##### Compared with $sklearn.linear\\_model.LogisticRegression$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logreg\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test sets and variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtestset = [X_test,X_test2,X_test3]\n",
    "ytestset = [y_test,y_test2,y_test3]\n",
    "\n",
    "lmbd=1\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = logreg.Logistic_Regression(learn_rate=0.1, iterations=iterations, normalize=False, lmbd=lmbd)\n",
    "regr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "z_train = regr.predict(X_train,binary=False)\n",
    "acc_train1 = regr.accuracy(y_train,z_train)\n",
    "mse_train1 = np.sum(np.abs(z_train-y_train)**2)/y_train.shape[0]\n",
    "# Test\n",
    "acc_test1 = []\n",
    "mse_test1 = []\n",
    "for i,test in enumerate(Xtestset):\n",
    "    z_test = regr.predict(test,binary=False)\n",
    "    acc_test1.append(regr.accuracy(ytestset[i],z_test))\n",
    "    mse_test1.append(np.sum(np.abs(ytestset[i]-z_test)**2)/ytestset[i].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test of sklearn model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lregr = LogisticRegression(random_state=1,verbose=0,max_iter=iterations,tol=1E-5, solver='lbfgs')\n",
    "Lregr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "L_z_train = Lregr.predict(X_train)\n",
    "acc_train2 = regr.accuracy(y_train,L_z_train)\n",
    "mse_train2 = np.sum(np.abs(L_z_train-y_train)**2)/y_train.shape[0]\n",
    "\n",
    "# Test\n",
    "acc_test2 = []\n",
    "mse_test2 = []\n",
    "for i,test in enumerate(Xtestset):\n",
    "    L_z_test = Lregr.predict(test)\n",
    "    acc_test2.append(Lregr.score(test,ytestset[i]))\n",
    "    mse_test2.append(np.sum(np.abs(ytestset[i]-L_z_test)**2)/ytestset[i].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   {:<10}{:<15}{:<15}'.format('','Our model', 'sklearn model'))\n",
    "print('\\nTrain set :\\n')\n",
    "print('   {:<10}{:<15}{:<15}'.format('Accuracy',acc_train1,acc_train2))\n",
    "print('   {:>8}  {:<15}{:<15}'.format('MSE',mse_train1,mse_train2))\n",
    "for i in range(len(acc_test1)):\n",
    "    print('\\nTest set {}:\\n'.format(i+1))\n",
    "    #print('{:<10}{:<15}{:<15}'.format('','Our model', 'sklearn model'))\n",
    "    print('   {:<10}{:<15}{:<15}'.format('Accuracy',acc_test1[i],acc_test2[i]))\n",
    "    print('   {:>8}  {:<15}{:<15}'.format('MSE',mse_test1[i],mse_test2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 150000\n",
    "data[state][:].shape, classes[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10000\n",
    "rand = np.random.randint(0,classes.shape[0]-1,num)\n",
    "pred_data = np.zeros((num,data[0][:].shape[0]+1))\n",
    "state = np.zeros(num)\n",
    "for i,n in enumerate(rand):\n",
    "    pred_data[i][0] = 1\n",
    "    pred_data[i][1:] = data[n][:]\n",
    "    state[i] = classes[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = regr.predict(pred_data,binary=False)\n",
    "pred2 = Lregr.predict(pred_data)\n",
    "print('Our average correct:     ',np.sum(np.isclose(pred,state))/num)\n",
    "print('sklearn average correct: ',np.sum(np.isclose(pred2,state))/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.astype(int))\n",
    "print(pred2)\n",
    "print(state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
