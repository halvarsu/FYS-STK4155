{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload  2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_analytical(point):\n",
    "    x,t = point\n",
    "    return np.exp(-np.pi**2*t)*np.sin(np.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "x0 = 0\n",
    "xN = 1\n",
    "Nx = 10\n",
    "Nt_min = 10\n",
    "\n",
    "# s = Dt/Dx^2\n",
    "# 0 \\geq s \\geq 1/2 for stability\n",
    "# => Dt = Dx^2 /2\n",
    "\n",
    "dx = (xN - x0)/(Nx-1)\n",
    "\n",
    "dt = dx**2 / 2\n",
    "dt = np.min([T/(Nt_min - 1), dx**2/2])\n",
    "Nt = int(T/dt) + 1\n",
    "\n",
    "print(dt, Nt)\n",
    "\n",
    "r = dt/dx**2\n",
    "factor = (1-2*r)\n",
    "\n",
    "x = np.linspace(x0, xN, Nx)\n",
    "times = np.linspace(0,  T,  Nt)\n",
    "\n",
    "u = np.zeros((Nt,Nx))\n",
    "u[0] = np.sin(np.pi*x)\n",
    "\n",
    "u_a = np.zeros((Nt,Nx))\n",
    "u_a[0] = np.sin(np.pi*x)\n",
    "\n",
    "for j,t in enumerate(times):\n",
    "    if j == 0:\n",
    "        continue\n",
    "    # u[j,0] = 0\n",
    "    # u[j,-1] = 0\n",
    "    for i in range(1,Nx-1):\n",
    "        u[j,i] = u[j-1,i]*factor + r * (u[j-1,i+1] + u[j-1, i-1])\n",
    "        \n",
    "    u_a[j] = g_analytical((x,t))\n",
    "    # u_a[j] = np.exp(-np.pi**2*t) * np.sin(np.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,[ax1,ax2,ax3] = plt.subplots(3, figsize = [8,10])\n",
    "\n",
    "time_res = 1\n",
    "ax1.set_title('Forward Euler')\n",
    "m = ax1.imshow(u[::time_res].T)\n",
    "plt.colorbar(m,ax=ax1)\n",
    "\n",
    "ax2.set_title('Analytical')\n",
    "m = ax2.imshow(u_a[::time_res].T)\n",
    "plt.colorbar(m,ax=ax2)\n",
    "\n",
    "ax3.set_title('Relative Error')\n",
    "m = ax3.imshow((np.abs(u_a - u)/u)[::time_res].T)\n",
    "plt.colorbar(m,ax=ax3);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using deep neural networks with Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def deep_neural_network(deep_params, x):\n",
    "    # x is now a point and a 1D numpy array; make it a column vector\n",
    "    num_coordinates = np.size(x,0)\n",
    "    x = x.reshape(num_coordinates,-1)\n",
    "    num_points = np.size(x,1)\n",
    "    # N_hidden is the number of hidden layers\n",
    "    N_hidden = np.size(deep_params) - 1 # -1 since params consist of parameters to all the hidden\n",
    "    # Assume that the input layer does nothing to the input x\n",
    "    x_input = x\n",
    "    x_prev = x_input\n",
    "    ## Hidden layers:\n",
    "    for l in range(N_hidden):\n",
    "        # From the list of parameters P; find the correct weigths and bias for this layer\n",
    "        w_hidden = deep_params[l]\n",
    "        # Add a row of ones to include bias\n",
    "        x_prev = np.concatenate((np.ones((1,num_points)), x_prev ), axis = 0)\n",
    "        z_hidden = np.matmul(w_hidden, x_prev)\n",
    "        x_hidden = sigmoid(z_hidden)\n",
    "        # Update x_prev such that next layer can use the output from this layer\n",
    "        x_prev = x_hidden\n",
    "    ## Output layer:\n",
    "    # Get the weights and bias for this layer\n",
    "    w_output = deep_params[-1]\n",
    "    # Include bias:\n",
    "    x_prev = np.concatenate((np.ones((1,num_points)), x_prev), axis = 0)\n",
    "    z_output = np.matmul(w_output, x_prev)\n",
    "    x_output = z_output\n",
    "    return x_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trial function:\n",
    "def u(x):\n",
    "    return np.sin(np.pi*x)\n",
    "def g_trial(point,P):\n",
    "    x,t = point\n",
    "    return (1-t)*u(x) + x*(1-x)*t*deep_neural_network(P,point)\n",
    "\n",
    "# The right side of the ODE:\n",
    "def f(point):\n",
    "    return 0.\n",
    "# The cost function:\n",
    "def cost_function(P, x, t):\n",
    "    cost_sum = 0\n",
    "    g_t_jacobian_func = jacobian(g_trial)\n",
    "    g_t_hessian_func = hessian(g_trial)\n",
    "    for x_ in x:\n",
    "        for t_ in t:\n",
    "            point = np.array([x_,t_])\n",
    "            g_t = g_trial(point,P)\n",
    "            g_t_jacobian = g_t_jacobian_func(point,P)\n",
    "            g_t_hessian = g_t_hessian_func(point,P)\n",
    "            g_t_dt = g_t_jacobian[1]\n",
    "            g_t_d2x = g_t_hessian[0][0]\n",
    "            func = f(point)\n",
    "            err_sqr = ( (g_t_dt - g_t_d2x) - func)**2\n",
    "            cost_sum += err_sqr\n",
    "    return cost_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up a function for training the network to solve for the equation\n",
    "def solve_pde_deep_neural_network(x,t, num_neurons, num_iter, lmb):\n",
    "    ## Set up initial weigths and biases\n",
    "    N_hidden = np.size(num_neurons)\n",
    "    ## Set up initial weigths and biases\n",
    "    # Initialize the list of parameters:\n",
    "    P = [None]*(N_hidden + 1) # + 1 to include the output layer\n",
    "    P[0] = npr.randn(num_neurons[0], 2 + 1 ) # 2 since we have two points, +1 to include bias\n",
    "    for l in range(1,N_hidden):\n",
    "        P[l] = npr.randn(num_neurons[l], num_neurons[l-1] + 1) # +1 to include bias\n",
    "    # For the output layer\n",
    "    P[-1] = npr.randn(1, num_neurons[-1] + 1 ) # +1 since bias is included\n",
    "    print('Initial cost: ',cost_function(P, x, t))\n",
    "    cost_function_grad = grad(cost_function,0)\n",
    "    # Let the update be done num_iter times\n",
    "    for i in range(num_iter):\n",
    "        cost_grad = cost_function_grad(P, x , t)\n",
    "        for l in range(N_hidden+1):\n",
    "            P[l] = P[l] - lmb * cost_grad[l]\n",
    "    print('Final cost: ',cost_function(P, x, t))\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian,hessian,grad\n",
    "import autograd.numpy.random as npr\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "\n",
    "### Use the neural network:\n",
    "npr.seed(15)\n",
    "\n",
    "## Decide the vales of arguments to the function to solve\n",
    "Nx = 10; Nt = 10\n",
    "x = np.linspace(0, 1, Nx)\n",
    "t = np.linspace(0,1,Nt)\n",
    "\n",
    "## Set up the parameters for the network\n",
    "num_hidden_neurons = [100, 25]\n",
    "num_iter = 250\n",
    "lmb = 0.01\n",
    "\n",
    "P = solve_pde_deep_neural_network(x,t, num_hidden_neurons, num_iter, lmb)\n",
    "\n",
    "## Store the results\n",
    "g_dnn_ag = np.zeros((Nx, Nt))\n",
    "G_analytical = np.zeros((Nx, Nt))\n",
    "\n",
    "for i,x_ in enumerate(x):\n",
    "    for j, t_ in enumerate(t):\n",
    "        point = np.array([x_, t_])\n",
    "        g_dnn_ag[i,j] = g_trial(point,P)\n",
    "        G_analytical[i,j] = g_analytic(point)\n",
    "\n",
    "# Find the map difference between the analytical and the computed solution\n",
    "diff_ag = np.abs(g_dnn_ag - G_analytical)\n",
    "\n",
    "print('Max absolute difference between the analytical solution and the network: %g'%np.max(diff_ag))\n",
    "## Plot the solutions in two dimensions, that being in position and time\n",
    "                                                                                           \n",
    "T,X = np.meshgrid(t,x)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Solution from the deep neural network w/ %d layer'%len(num_hidden_neurons))\n",
    "s = ax.plot_surface(T,X,g_dnn_ag,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Analytical solution')\n",
    "s = ax.plot_surface(T,X,G_analytical,linewidth=0,antialiased=False,cmap=cm.viridis)\n",
    "ax.set_xlabel('Time $t$')\n",
    "ax.set_ylabel('Position $x$');\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Difference')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
